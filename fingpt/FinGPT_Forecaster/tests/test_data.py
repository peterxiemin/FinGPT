import pytest
import pandas as pd
import os
import json
from unittest.mock import patch
from data import get_returns, get_news, get_basics, prepare_data_for_symbol

# ç»Ÿä¸€æµ‹è¯•æ—¶é—´ï¼šé€‰æ‹©æœ€è¿‘ä¸€ä¸ªç¨³å®šçš„æ—¶é—´çª—å£ï¼ˆçº¦6å‘¨ï¼‰ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®ç”Ÿæˆ Prompt
TEST_START = "2026-01-01"
TEST_END = "2026-02-15"

def test_get_returns_format():
    """æµ‹è¯•è‚¡ä»·è·å–å‡½æ•°æ˜¯å¦èƒ½æ­£ç¡®å¤„ç† yfinance çš„ MultiIndex æ ¼å¼"""
    symbol = "AAPL"
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£
    df = get_returns(symbol, TEST_START, TEST_END)
    
    required_columns = ['Start Date', 'Start Price', 'End Date', 'End Price', 'Weekly Returns', 'Bin Label']
    for col in required_columns:
        assert col in df.columns, f"ç¼ºå¤±å…³é”®åˆ—: {col}"
    
    assert len(df) > 0, "ä¸åº”è¿”å›ç©ºæ•°æ®"
    # yfinance æœ€è¿‘è¿”å›çš„æ˜¯ Series æˆ– floatï¼Œå–å†³äº pandas ç‰ˆæœ¬
    val = df['Weekly Returns'].iloc[0]
    assert isinstance(val, (float, int)) or hasattr(val, '__float__')
    assert isinstance(df['Bin Label'].iloc[0], str)

def test_get_news_content():
    """æµ‹è¯•æ–°é—»è·å–åŠŸèƒ½ï¼ˆçœŸå® API è°ƒç”¨ï¼‰"""
    symbol = "AAPL"
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£
    df = get_returns(symbol, TEST_START, TEST_END)
    
    # è·å–æ–°é—»
    df_with_news = get_news(symbol, df)
    
    assert 'News' in df_with_news.columns
    # éªŒè¯æ–°é—»å†…å®¹æ˜¯å¦ä¸ºåˆæ³•çš„ JSON å­—ç¬¦ä¸²
    news_json = df_with_news['News'].iloc[0]
    news_data = json.loads(news_json)
    assert isinstance(news_data, list)
    
    # åªè¦ API è°ƒé€šï¼Œæˆ‘ä»¬éªŒè¯ç»“æ„
    if len(news_data) > 0:
        assert 'headline' in news_data[0]
        assert 'date' in news_data[0]

def test_get_basics_content():
    """æµ‹è¯•åŸºæœ¬é¢è·å–åŠŸèƒ½ï¼ˆçœŸå® API è°ƒç”¨ï¼‰"""
    symbol = "AAPL"
    df = get_returns(symbol, TEST_START, TEST_END)
    
    # æµ‹è¯•è·å–åŸºæœ¬é¢è´¢åŠ¡æ•°æ®
    df_with_basics = get_basics(symbol, df, TEST_START, always=True)
    
    assert 'Basics' in df_with_basics.columns
    basics_json = df_with_basics['Basics'].iloc[0]
    basics_data = json.loads(basics_json)
    
    # éªŒè¯ç»“æ„
    assert isinstance(basics_data, dict)
    if basics_data:
        assert 'period' in basics_data

def test_prepare_data_full_lifecycle():
    """æµ‹è¯•å®Œæ•´çš„æ•°æ®å‡†å¤‡ç”Ÿå‘½å‘¨æœŸ"""
    symbol = "AAPL"
    # ä½¿ç”¨é¡¹ç›® test ä¸‹çš„ä¸“é—¨æ•°æ®ç›®å½•
    data_dir = os.path.join(os.path.dirname(__file__), "output_data")
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£
    start = TEST_START
    end = TEST_END
    
    df = prepare_data_for_symbol(symbol, data_dir, start, end, with_basics=True)
    
    # éªŒè¯è¿”å›çš„ DataFrame
    assert not df.empty
    assert 'News' in df.columns
    assert 'Basics' in df.columns
    
    # éªŒè¯ CSV æ–‡ä»¶æ˜¯å¦å®é™…å†™å…¥
    expected_csv = os.path.join(data_dir, f"{symbol}_{start}_{end}.csv")
    assert os.path.exists(expected_csv)
    
    # --- æ·»åŠ æ–‡ä»¶åˆ†æ ---
    analyze_output_csv(expected_csv)
    
    return symbol, data_dir, start, end

def analyze_output_csv(file_path):
    """å¯¹ç”Ÿæˆçš„åŸå§‹æ•°æ® CSV è¿›è¡Œç»Ÿè®¡åˆ†æ"""
    print(f"\n" + "="*50)
    print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡: {os.path.basename(file_path)}")
    df = pd.read_csv(file_path)
    print(f"â€¢ æ€»æ ·æœ¬é‡ (å‘¨): {len(df)}")
    print(f"â€¢ æ—¶é—´èŒƒå›´: {df['Start Date'].min()} -> {df['End Date'].max()}")
    print(f"â€¢ æ¶¨è·Œåˆ†å¸ƒ: {df['Bin Label'].value_counts().to_dict()}")
    
    # æ£€æŸ¥æ–°é—»å¯†åº¦
    news_lens = df['News'].apply(lambda x: len(json.loads(x)))
    print(f"â€¢ å¹³å‡æ¯å‘¨æŠ“å–æ–°é—»: {news_lens.mean():.1f} æ¡")
    
    # æ£€æŸ¥åŸºæœ¬é¢è¦†ç›–
    has_basics = df['Basics'].apply(lambda x: x != '{}' and x != '"{}"').sum()
    print(f"â€¢ è´¢åŠ¡æ•°æ®è¦†ç›–ç‡: {has_basics}/{len(df)} å‘¨")
    print("="*50)

def test_query_gpt4_lifecycle():
    """æµ‹è¯• LLM é¢„æµ‹åˆ†æå…¨æµç¨‹ (ä¾èµ– DeepSeek/OpenRouter)"""
    from data import query_gpt4
    
    # 1. é¦–å…ˆå‡†å¤‡åŸºç¡€æ•°æ®
    symbol, data_dir, start, end = test_prepare_data_full_lifecycle()
    
    # 2. è°ƒç”¨ LLM è¿›è¡Œåˆ†æé¢„æµ‹
    # ä¸ºäº†æµ‹è¯•é€Ÿåº¦ï¼Œæˆ‘ä»¬åªå¯¹ä¸€ä¸¤å‘¨çš„æ•°æ®è¿›è¡Œè¯¢é—®
    model_name = os.environ.get("OPENAI_MODEL", "deepseek/deepseek-chat")
    model_suffix = model_name.split('/')[-1]
    
    print(f"\nå¼€å§‹æµ‹è¯• LLM é¢„æµ‹ (DeepSeek)... ç›®æ ‡æ–‡ä»¶: {symbol}_{start}_{end}_{model_suffix}.csv")
    query_gpt4([symbol], data_dir, start, end, min_past_weeks=1, max_past_weeks=1, with_basics=True)
    
    # 3. éªŒè¯é¢„æµ‹ç»“æœæ–‡ä»¶
    prediction_csv = os.path.join(data_dir, f"{symbol}_{start}_{end}_{model_suffix}.csv")
    assert os.path.exists(prediction_csv), "é¢„æµ‹ç»“æœ CSV æœªç”Ÿæˆ"
    
    pred_df = pd.read_csv(prediction_csv)
    assert len(pred_df) > 0, "é¢„æµ‹ç»“æœä¸ºç©º"
    assert "prompt" in pred_df.columns
    assert "answer" in pred_df.columns
    
    # éªŒè¯ç­”æ¡ˆå†…å®¹æ˜¯å¦åŒ…å«é¢„æœŸçš„å…³é”®è¯ (æ¯”å¦‚åˆ†ææˆ–é¢„æµ‹)
    last_answer = pred_df['answer'].iloc[-1]
    assert len(str(last_answer)) > 100, "LLM å›ç­”è¿‡çŸ­ï¼Œå¯èƒ½è°ƒç”¨å¤±è´¥"
    
    # --- æ·»åŠ  LLM åˆ†ææ–‡ä»¶çš„æ·±åº¦ç»Ÿè®¡ ---
    analyze_gpt4_csv(prediction_csv)

def analyze_gpt4_csv(file_path):
    """å¯¹ LLM ç”Ÿæˆçš„åˆ†ææŠ¥å‘Šè¿›è¡Œè´¨é‡æ ¸å¯¹"""
    print(f"\n" + "ğŸ¤– LLM ç”Ÿæˆè´¨é‡æŠ¥å‘Š ".center(50, "="))
    df = pd.read_csv(file_path)
    print(f"â€¢ ç”Ÿæˆåˆ†ææ€»æ•°: {len(df)}")
    
    # ç»“æ„åŒ–éªŒè¯
    last_ans = str(df['answer'].iloc[-1])
    required = ["[Positive Developments]", "[Potential Concerns]", "[Prediction & Analysis]"]
    found = [r for r in required if r in last_ans]
    
    print(f"â€¢ ç»“æ„å®Œæ•´æ€§: {len(found)}/{len(required)}")
    if len(found) == len(required):
        print("  âœ… æ®µè½ç»“æ„ç¬¦åˆè¯åˆ¸åˆ†æå¸ˆè§„èŒƒ")
    else:
        print(f"  âŒ ç¼ºå¤±æ®µè½: {[r for r in required if r not in found]}")
        
    # é•¿åº¦éªŒè¯
    avg_len = df['answer'].apply(lambda x: len(str(x))).mean()
    print(f"â€¢ å¹³å‡åˆ†æå­—æ•°: {int(avg_len)} å­—")
    print("="*50 + "\n")

def test_bin_mapping_logic():
    """æµ‹è¯•æ¶¨è·Œå¹…åˆ†æ¡¶é€»è¾‘"""
    from data import bin_mapping

    assert bin_mapping(0.005) == "U1"
    assert bin_mapping(-0.023) == "D3"
    assert bin_mapping(0.06) == "U5+"
    assert bin_mapping(-0.15) == "D5+"


# ---------------------------------------------------------------------------
# create_dataset è¾¹ç•Œæµ‹è¯•ï¼ˆmock gpt4_to_llamaï¼Œä¸è°ƒç”¨çœŸå® APIï¼‰
# ---------------------------------------------------------------------------

_MOCK_DATA_3 = {
    "prompt":  ["prompt_A", "prompt_B", "prompt_C"],
    "answer":  ["answer_A", "answer_B", "answer_C"],
    "period":  ["2026-01-08_2026-01-15", "2026-01-15_2026-01-22", "2026-01-22_2026-01-29"],
    "label":   ["U1", "D2", "U3"],
}


def test_create_dataset_empty_test_split_ok():
    """train_ratio=1.0 æ—¶æ‰€æœ‰æ ·æœ¬è¿› trainï¼Œtest åº”ä¸ºç©º schemaï¼Œä¸åº” crashã€‚"""
    from data import create_dataset

    with patch("data.gpt4_to_llama", return_value=_MOCK_DATA_3):
        result = create_dataset(
            ["AAPL"], "/fake/dir", "2026-01-01", "2026-02-15",
            train_ratio=1.0, with_basics=True
        )

    assert len(result["train"]) == 3, "train åº”å«å…¨éƒ¨ 3 æ¡æ ·æœ¬"
    assert len(result["test"]) == 0,  "test åº”ä¸ºç©ºï¼ˆä¸æ˜¯ crashï¼‰"
    assert set(result["test"].column_names) == set(result["train"].column_names), \
        "test ä¸ train åº”æœ‰ç›¸åŒçš„ schema"


def test_create_dataset_normal_split():
    """train_ratio=0.8ï¼Œ3 æ¡æ ·æœ¬ â†’ train=2, test=1ã€‚"""
    from data import create_dataset

    with patch("data.gpt4_to_llama", return_value=_MOCK_DATA_3):
        result = create_dataset(
            ["AAPL"], "/fake/dir", "2026-01-01", "2026-02-15",
            train_ratio=0.8, with_basics=True
        )

    # round(0.8 * 3) = 2
    assert len(result["train"]) == 2
    assert len(result["test"]) == 1


def test_create_dataset_no_samples_raises():
    """æ‰€æœ‰ symbol å‡è¿”å› 0 æ¡ prompt æ—¶ï¼Œåº”æŠ›å‡º ValueErrorï¼ˆè€Œé Trainer å†…éƒ¨ crashï¼‰ã€‚"""
    from data import create_dataset

    empty = {"prompt": [], "answer": [], "period": [], "label": []}
    with patch("data.gpt4_to_llama", return_value=empty):
        with pytest.raises(ValueError, match="No training samples"):
            create_dataset(
                ["AAPL", "MSFT"], "/fake/dir", "2026-01-01", "2026-01-08"
            )
